# FAISS ho·∫∑c ChromaDB 

import os
import numpy as np
import faiss
import pickle
from typing import List, Dict, Any, Tuple, Optional
from app.search.embedder import get_embedder
from app.files.file_loader import load_file

class VectorStore:
    """
    L∆∞u tr·ªØ v√† t√¨m ki·∫øm vector embedding s·ª≠ d·ª•ng FAISS
    """
    
    def __init__(self, index_path: str = "data/vector_store/faiss_index"):
        self.index_path = index_path
        self.index = None
        self.metadata = []  # L∆∞u th√¥ng tin ƒëi k√®m v·ªõi m·ªói embedding
        self.embedder = get_embedder()
        self._ensure_directory()
        
    def _ensure_directory(self):
        """ƒê·∫£m b·∫£o th∆∞ m·ª•c l∆∞u tr·ªØ index t·ªìn t·∫°i"""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
    
    def _init_index(self, dimension: int):
        """Kh·ªüi t·∫°o FAISS index v·ªõi k√≠ch th∆∞·ªõc x√°c ƒë·ªãnh"""
        self.index = faiss.IndexFlatL2(dimension)  # L2 distance
    
    def add_documents(self, documents: List[str], metadatas: List[Dict[str, Any]] = None):
        """
        Th√™m c√°c ƒëo·∫°n vƒÉn b·∫£n v√†o index
        
        Args:
            documents: Danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n
            metadatas: Th√¥ng tin ƒëi k√®m v·ªõi m·ªói ƒëo·∫°n vƒÉn b·∫£n
        """
        if not documents:
            return
            
        embeddings = self.embedder.embed_texts(documents)
        
        # Kh·ªüi t·∫°o index n·∫øu ch∆∞a c√≥
        if self.index is None:
            self._init_index(self.embedder.get_vector_size())
        
        # Th√™m embeddings v√†o index
        self.index.add(np.array(embeddings).astype('float32'))
        
        # L∆∞u metadata
        if metadatas:
            self.metadata.extend(metadatas)
        else:
            self.metadata.extend([{"text": doc} for doc in documents])
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        T√¨m ki·∫øm c√°c ƒëo·∫°n vƒÉn b·∫£n t∆∞∆°ng t·ª± v·ªõi c√¢u query
        
        Args:
            query: C√¢u truy v·∫•n
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            
        Returns:
            Danh s√°ch c√°c k·∫øt qu·∫£ ph√π h·ª£p nh·∫•t
        """
        if self.index is None or self.index.ntotal == 0:
            print(f"‚ùå Vector store kh√¥ng c√≥ d·ªØ li·ªáu. Index l√† None: {self.index is None}")
            return []
            
        print(f"üîç T√¨m ki·∫øm trong vector store v·ªõi query: '{query}'")
        print(f"üìä Vector store c√≥ {self.index.ntotal} embedding vectors")
            
        # T·∫°o embedding cho c√¢u query
        query_embedding = self.embedder.embed_text(query)
        query_embedding = np.array([query_embedding]).astype('float32')
        
        # T√¨m ki·∫øm top_k k·∫øt qu·∫£ g·∫ßn nh·∫•t
        distances, indices = self.index.search(query_embedding, top_k)
        
        results = []
        print(f"üî¢ K·∫øt qu·∫£ indices: {indices[0]}")
        print(f"üìè K·∫øt qu·∫£ distances: {distances[0]}")
        
        for i, idx in enumerate(indices[0]):
            if idx != -1 and idx < len(self.metadata):
                result = dict(self.metadata[idx])
                result["distance"] = float(distances[0][i])
                result["score"] = 1.0 / (1.0 + float(distances[0][i]))  # Convert kho·∫£ng c√°ch th√†nh ƒëi·ªÉm s·ªë
                print(f"üìÑ T√¨m th·∫•y: {result.get('file_name')} (score: {result['score']:.4f})")
                results.append(result)
        
        if not results:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p trong vector store")
            
        return results
    
    def save(self):
        """L∆∞u index v√† metadata v√†o file"""
        if self.index is None:
            return False
            
        try:
            # L∆∞u FAISS index
            faiss.write_index(self.index, f"{self.index_path}.faiss")
            
            # L∆∞u metadata
            with open(f"{self.index_path}.metadata", "wb") as f:
                pickle.dump(self.metadata, f)
                
            return True
        except Exception as e:
            print(f"L·ªói khi l∆∞u vector store: {str(e)}")
            return False
    
    def load(self) -> bool:
        """T·∫£i index v√† metadata t·ª´ file"""
        if not os.path.exists(f"{self.index_path}.faiss") or not os.path.exists(f"{self.index_path}.metadata"):
            return False
            
        try:
            # T·∫£i FAISS index
            self.index = faiss.read_index(f"{self.index_path}.faiss")
            
            # T·∫£i metadata
            with open(f"{self.index_path}.metadata", "rb") as f:
                self.metadata = pickle.load(f)
                
            return True
        except Exception as e:
            print(f"L·ªói khi t·∫£i vector store: {str(e)}")
            self.index = None
            self.metadata = []
            return False

# Kh·ªüi t·∫°o singleton
_vector_store = None

def get_vector_store() -> VectorStore:
    """H√†m ti·ªán √≠ch ƒë·ªÉ l·∫•y instance c·ªßa VectorStore"""
    global _vector_store
    if _vector_store is None:
        _vector_store = VectorStore()
        _vector_store.load()  # C·ªë g·∫Øng t·∫£i index n·∫øu c√≥
    return _vector_store

def index_documents(file_paths: List[str], force_reindex: bool = False) -> bool:
    """
    ƒê√°nh ch·ªâ m·ª•c (index) cho c√°c file vƒÉn b·∫£n
    
    Args:
        file_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n c√°c file
        force_reindex: ƒê√°nh ch·ªâ m·ª•c l·∫°i n·∫øu ƒë√£ t·ªìn t·∫°i
        
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    from app.files.file_parser import chunk_text
    
    vector_store = get_vector_store()
    if vector_store.index is not None and not force_reindex:
        return True
        
    texts = []
    metadata = []
    
    for file_path in file_paths:
        try:
            content = load_file(file_path)
            if not content:
                continue
                
            # Chia n·ªôi dung th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n
            chunks = chunk_text(content)
            
            for i, chunk in enumerate(chunks):
                texts.append(chunk)
                metadata.append({
                    "file_path": file_path,
                    "file_name": os.path.basename(file_path),
                    "chunk_id": i,
                    "total_chunks": len(chunks)
                })
                
        except Exception as e:
            print(f"L·ªói khi ƒë√°nh ch·ªâ m·ª•c file {file_path}: {str(e)}")
    
    if texts:
        vector_store.add_documents(texts, metadata)
        vector_store.save()
        return True
    
    return False