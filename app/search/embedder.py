from typing import List, Union, Optional
import os
import numpy as np
from sentence_transformers import SentenceTransformer

class TextEmbedder:
    """
    Module táº¡o embedding tá»« vÄƒn báº£n sá»­ dá»¥ng sentence-transformers.
    Há»— trá»£ cháº¡y offline sau khi Ä‘Ã£ táº£i mÃ´ hÃ¬nh.
    """
    
    def __init__(
        self, 
        model_name: str = "all-MiniLM-L6-v2",
        cache_folder: Optional[str] = None,
        device: str = "cpu"
    ):
        """
        Khá»Ÿi táº¡o mÃ´ hÃ¬nh embedding.
        
        Args:
            model_name: TÃªn mÃ´ hÃ¬nh sentence-transformers (máº·c Ä‘á»‹nh: all-MiniLM-L6-v2). CÃ³ thá»ƒ dÃ¹ng "paraphrase-multilingual-MiniLM-L12-v2" Ä‘á»ƒ há»— trá»£ tiáº¿ng Viá»‡t tá»‘t hÆ¡n
            cache_folder: ThÆ° má»¥c lÆ°u cache mÃ´ hÃ¬nh (Ä‘á»ƒ cháº¡y offline)
            device: Thiáº¿t bá»‹ tÃ­nh toÃ¡n ("cpu" hoáº·c "cuda" náº¿u cÃ³ GPU)
        """
        try:
            self.model = SentenceTransformer(
                model_name_or_path=model_name,
                cache_folder=cache_folder,
                device=device
            )
            self.vector_size = self.model.get_sentence_embedding_dimension()
            print(f"âœ“ ÄÃ£ táº£i mÃ´ hÃ¬nh embedding: {model_name} (kÃ­ch thÆ°á»›c vector: {self.vector_size})")
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh embedding: {str(e)}")
            raise
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        Táº¡o embedding cho má»™t Ä‘oáº¡n vÄƒn báº£n.
        
        Args:
            text: Äoáº¡n vÄƒn báº£n cáº§n táº¡o embedding
            
        Returns:
            Vector embedding dáº¡ng numpy array
        """
        if not text or not text.strip():
            # Tráº£ vá» vector 0 náº¿u text rá»—ng
            return np.zeros(self.vector_size)
        
        try:
            return self.model.encode(text, show_progress_bar=False)
        except Exception as e:
            print(f"âŒ Lá»—i khi táº¡o embedding: {str(e)}")
            return np.zeros(self.vector_size)
    
    def embed_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        Táº¡o embedding cho nhiá»u Ä‘oáº¡n vÄƒn báº£n.
        
        Args:
            texts: Danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n
            batch_size: Sá»‘ lÆ°á»£ng vÄƒn báº£n xá»­ lÃ½ trong má»—i batch
            
        Returns:
            Ma tráº­n cÃ¡c vector embedding
        """
        if not texts:
            return np.array([])
        
        # Lá»c bá» cÃ¡c text rá»—ng
        texts = [t for t in texts if t and t.strip()]
        
        if not texts:
            return np.array([])
        
        try:
            return self.model.encode(
                texts, 
                batch_size=batch_size,
                show_progress_bar=len(texts) > 10
            )
        except Exception as e:
            print(f"âŒ Lá»—i khi táº¡o embedding batch: {str(e)}")
            return np.zeros((len(texts), self.vector_size))
    
    def get_vector_size(self) -> int:
        """Tráº£ vá» kÃ­ch thÆ°á»›c cá»§a vector embedding"""
        return self.vector_size
    
    def is_ready(self) -> bool:
        """Kiá»ƒm tra xem mÃ´ hÃ¬nh Ä‘Ã£ sáºµn sÃ ng chÆ°a"""
        return self.model is not None


# HÃ m tiá»‡n Ã­ch Ä‘á»ƒ táº¡o embedder singleton
_embedder_instance = None

def get_embedder(
    model_name: str = "all-MiniLM-L6-v2", 
    cache_folder: Optional[str] = None,
    force_new: bool = False
) -> TextEmbedder:
    """
    Tráº£ vá» instance cá»§a TextEmbedder (singleton pattern).
    
    Args:
        model_name: TÃªn mÃ´ hÃ¬nh sentence-transformers
        cache_folder: ThÆ° má»¥c lÆ°u cache mÃ´ hÃ¬nh
        force_new: Táº¡o má»›i instance ngay cáº£ khi Ä‘Ã£ cÃ³
        
    Returns:
        Instance cá»§a TextEmbedder
    """
    global _embedder_instance
    
    if _embedder_instance is None or force_new:
        print(f"ğŸ”„ Khá»Ÿi táº¡o embedder vá»›i model: {model_name}")
        _embedder_instance = TextEmbedder(
            model_name=model_name,
            cache_folder=cache_folder
        )
    
    return _embedder_instance
